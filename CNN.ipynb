{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Convolutional Neural Network.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hzKV5-2nv4dA"
      },
      "source": [
        "**Initialization**\n",
        "* I use these 3 lines of code on top of my each Notebooks because it will help to prevent any problems while reloading and reworking on a Project or Problem. And the third line of code helps to make visualization within the Notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bj-NI7PivXIP"
      },
      "source": [
        "#@ Initialization:\n",
        "%reload_ext autoreload\n",
        "%autoreload 2\n",
        "%matplotlib inline"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "emucWoKGwTKW"
      },
      "source": [
        "**Downloading the Dependencies**\n",
        "* I have downloaded all the Libraries and Dependencies required for this Project in one particular cell."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5TLnl5GHwQx4"
      },
      "source": [
        "#@ Downloading the Libraries and Dependencies:\n",
        "# !pip install nlpia\n",
        "import os, glob\n",
        "from random import shuffle\n",
        "\n",
        "import numpy as np                                      # Module to work with Arrays.\n",
        "from keras.preprocessing import sequence                # Module to handle Padding Input.\n",
        "from keras.models import Sequential                     # Base Keras Neural Network Model.\n",
        "from keras.layers import Dense, Dropout, Activation     # Layers Objects to pile into Model.\n",
        "from keras.layers import Conv1D, GlobalMaxPool1D        # Convolutional Layer and MaxPooling.\n",
        "\n",
        "from nltk.tokenize import TreebankWordTokenizer         # Module for Tokenization.\n",
        "from gensim.models.keyedvectors import KeyedVectors\n",
        "from nlpia.loaders import get_data                      # Importing the NLPIA Package."
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KlazyhiY1opk"
      },
      "source": [
        "**Getting the Data**\n",
        "* I have used Google Colab for this Project so the process of downloading and reading the Data might be different in other platforms. I have used [**Large Moview Review Dataset**](https://ai.stanford.edu/~amaas/data/sentiment/) for this Project. This is a dataset for binary sentiment classification containing substantially more data. The Dataset has a set of 25,000 highly polar movie reviews for training and 25,000 for testing. There is additional unlabeled data for use as well. Raw text and already processed bag of words formats are provided."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oZSfbSTMy-Nt"
      },
      "source": [
        "#@ Getting the Data:\n",
        "def preprocess_data(filepath):\n",
        "  positive_path = os.path.join(filepath, \"pos\")\n",
        "  negative_path = os.path.join(filepath, \"neg\")\n",
        "  pos_label = 1\n",
        "  neg_label = 0\n",
        "  dataset = []\n",
        "  \n",
        "  for filename in glob.glob(os.path.join(positive_path, '*.txt')):                            # Positive Sentiment Dataset.\n",
        "    with open(filename, \"r\") as f:\n",
        "      dataset.append((pos_label, f.read()))\n",
        "  for filename in glob.glob(os.path.join(negative_path, '*.txt')):                            # Negative Sentiment Dataset.\n",
        "    with open(filename, \"r\") as f:\n",
        "      dataset.append((neg_label, f.read()))\n",
        "\n",
        "  shuffle(dataset)                                                                            # Shuffling the Dataset.\n",
        "  return dataset "
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lKeAvcUE-h1P"
      },
      "source": [
        "**Processing the Dataset**\n",
        "* I have manually downloaded the Dataset from [**Large Moview Review Dataset**](https://ai.stanford.edu/~amaas/data/sentiment/). I have used the small subset of Data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bxusBQ_ba3G-",
        "outputId": "c0f9078a-81f9-4b9c-b9d6-1675838fb7c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "#@ Processing the Dataset:\n",
        "PATH = \"/content/drive/My Drive/Colab Notebooks/Data/Smalltrain\"                     # Path to the Dataset.\n",
        "dataset = preprocess_data(PATH)                                                      # Processing the Dataset.\n",
        "\n",
        "#@ Inspecting the Dataset:\n",
        "dataset[:3]                                                                          # Inspecting the Dataset."
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0,\n",
              "  \"Anybody who has ever been a fan of the original series, or even has a clue about the storyline should be embarrassed by this series. The Borg does not come around until Q brings the Enterprise to the Gamma sector, the Klingons are NEVER seen until Kirk encounters them, the NCC-1701 was the FIRST ship to carry the Enterprise name....need I go on? Berman and Pilliar have made a mockery of Gene Roddenberry's creation. After he died, they only saw $$$$ and just went their own way. No wonder Majel Barrett was in every single episode of star trek until this series. I don't blame her for not being involved with this mess. Poor Bakula. He's a great actor, as are the entire cast. I like them all, but the storyline is tragic and ignores all of the precedents set by the original series. Just check the ratings. I think more people watched Deep Space 9 (which was untimely canceled).\"),\n",
              " (1,\n",
              "  'This first-rate western tale of the gold rush brings great excitement, romance, and James Stewart to the screen. \"The Far Country\" is the only one out of all five Stewart-Mann westerns that is often overlooked. Stewart, yet again, puts a new look on the ever-present personalities he had in the five Stewart-Mann westerns. Jeff Webster (Stewart) is uncaring, always looking out for himself, which is why he is so surprised when people are nice and kindly to him. Ironically, he does wear a bell on his saddle that he will not ride without. This displays that he might just care for one person- his sidekick, Ben Tatum, played by Walter Brennan, since Tatum is the one that gave it to him. Mann, yet again, puts a new look on the ever present personalities he put into the five Stewart-Mann westerns. He displays violence, excitement, plot twists, romance, and corruption. The story is that Jeff and Ben, through a series of events, wind up in the get rich quick town of Dawson, along with gold partners Calvet and Flippen, and no-good but beautiful Roman and her hired men. They are unable to leave, because crooked sheriff Mr. Gannon (McIntire) and his \"deputies\" will hang them, since the only way out is through Skagway, which is Gannon\\'s town. But, eventually, McIntire comes to them, but not to collect Stewart and/or his fine that he supposedly owes to the government. What is McIntire there for? He is there to cheat miners out of their claims and money. People are killed. A sheriff for Dawson is considered needed, and Calvet elects Stewart because he is good with a gun. Stewart, however, refuses the job, because he plans to get all the gold he can, and then pull out. He also refuses it because he does not like to help people, since law and order always gets somebody killed. So, Flippen is elected instead. A miner is killed because he tries to stand up to one of Gannon\\'s men, a purely evil, mustachioed fancy gunman named Madden, who carries two guns, played by Wilke. Flippen attempts to arrest Madden and see that justice be done, but he cannot stand up to him, so he becomes the town drunk. A man named Yukon replaces Flippen. Stewart and Tatum start to pull out, but are ambushed by Gannon\\'s men. Tatum is killed, and Stewart is wounded. Stewart finally realizes that he must do something, or Gannon will take over Dawson, set up his own rules, and it will become his town, just like Skagway. The audience also realizes what Stewart must do. Another thing that the audience realizes is that Stewart is the only thing that stands between the townspeople and Gannon. If Stewart leaves, Gannon would take over the town. If Stewart stays and keeps on not doing anything about it, the townspeople will be killed one by one mercilessly and uselessly. This is where a great scene occurs. Stewart walks into his cabin. He has a sling on his arm. For a few seconds, his gun, in the gunbelt, is hanging on a post beside his bed, the gun is close up, Stewart is in the background, just inside the door. He stares at it for a few seconds. He tosses the sling away. The sling lands on the back of a chair, and falls to the floor. This is symbolic, because he is throwing away his old life, which consisted of not caring about anybody but himself. He comes into his new life, of helping people when they need help. What ends the film is a guns-blazing, furious show of good against evil, and a genuinely feel-good feeling that everything will be alright.'),\n",
              " (1,\n",
              "  \"Tales from the Crypt: And All Through the House starts on Christmas Eve as Elizabeth (Mary Ellen Trainor) kills her husband Joseph (Marshall Bell), she drags his body outside ready to throw it down a well but while doing so misses an important news bulletin on the radio that says a homicidal maniac (Larry Drake) dressed as Santa Clause has escaped from a local mental asylum & has already killed several women with Elizabeth next on his list but she has other ideas & tries to turn the seemingly dangerous situation to her advantage...<br /><br />This Tales from the Crypt story was episode 2 from season 1, directed by the one of the show's regular executive producers Robert Zemeckis And All Through the House is a decent enough watch. The script by Fred Dekker was actually based on a story appearing in the comic 'The Vault of Horror' & was originally adapted to film as one episode from the Britsih horror anthology film Tales from the Crypt (1972) which starred Joan Collins as the murderous wife character here played by Ellen Trainor. This particular version is good enough but doesn't do anything different or special & is a bit too linear & predictable to be considered a classic. At only 25 minutes in length it certainly moves along at a good pace, the story is just about macabre enough & it generally provides decent entertainment & I quite liked the downbeat ending. This time there are Christmas themed opening & closing Crypt Keeper (John Kassir) segments complete with the usual puns.<br /><br />Director Zemeckis does a good job & there's a nice winterly atmosphere with a hint of Christmas influence as well. There's not much gore here, someone has a poker stuck in their head, someone's face is cut with an icicle, someone's arm is cut with an axe & there's some blood splatter but generally speaking it's not that graphic. The acting by a small cast is pretty good.<br /><br />And All Through the House isn't the best tale from the crypt but it's a decent one all the same, worth a watch but after the comic book story & original 1972 film version did we really need or even want this?\")]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wqmN_Mc-lClh"
      },
      "source": [
        "**Tokenization and Vectorization**\n",
        "* The next step is to perform the Tokenization and Vectorization of the Dataset. I will use Google news pretrained Model Vectors for the process of Vectorization. The Google News Word2vec Vocabulary includes some stopwords as well. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IB3wv7UfvtkF",
        "outputId": "c1c43aae-dc91-495b-916c-0ebc9818c9fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "#@ Tokenization and Vectorization:\n",
        "# !wget -c \"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\"                # Pretrained Word2vec Model.    \n",
        "\n",
        "word_vectors = KeyedVectors.load_word2vec_format(\"/content/GoogleNews-vectors-negative300.bin.gz\",           # Word2vec Model Vectors.\n",
        "                                       binary=True, limit=100000)\n",
        "\n",
        "#@ Function for Tokenization and Vectorization:\n",
        "def tokenize_and_vectorize(dataset):\n",
        "  tokenizer = TreebankWordTokenizer()                                  # Instantiating the Tokenizer.\n",
        "  vectorized_data = []\n",
        "  for sample in dataset:\n",
        "    tokens = tokenizer.tokenize(sample[1])                             # Process for Tokenization.\n",
        "    sample_vecs = []\n",
        "    for token in tokens:\n",
        "      try:\n",
        "        sample_vecs.append(word_vectors[token])                        # Process for Vectorization.\n",
        "      except KeyError:\n",
        "        pass\n",
        "    vectorized_data.append(sample_vecs)\n",
        "  \n",
        "  return vectorized_data                                               # Returning the Vectorized Data.\n",
        "\n",
        "#@ Function for Collecting the Target Labels:\n",
        "def collect_expected(dataset):\n",
        "  \"\"\" Collecting the Target Labels: 0 for Negative Review and 1 for Positive Review. \"\"\"\n",
        "  expected=[]\n",
        "  for sample in dataset:\n",
        "    expected.append(sample[0])\n",
        "  return expected\n",
        "\n",
        "#@ Tokenization and Vectorization:\n",
        "vectorized_data = tokenize_and_vectorize(dataset)\n",
        "expected = collect_expected(dataset)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:252: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJiAM9SZzx7z"
      },
      "source": [
        "**Splitting into Training and Testing.**\n",
        "* Now, I will split the above obtained Dataset into Training set and a Test set. I will split the Dataset into 80% for Training and 20% for Test set. The next code will bucket the Data into Training set X_train along with correct labels y_train and similarly into Test set X_test along with correct labels y_test."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fmtgb2HysvRD"
      },
      "source": [
        "#@ Splitting the Dataset into Training set and Test set:\n",
        "split_part = int(len(vectorized_data) * 0.8)\n",
        "\n",
        "#@ Training set:\n",
        "X_train = vectorized_data[:split_part]\n",
        "y_train = vectorized_data[:split_part]\n",
        "\n",
        "#@ Test set:\n",
        "X_test = vectorized_data[split_part:]\n",
        "y_test = vectorized_data[split_part:]"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XCmrmUQ41-ai"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}